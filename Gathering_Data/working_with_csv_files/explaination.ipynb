{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6951d4e1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "This video marks a progression in the machine learning process, shifting focus towards data acquisition following a previous discussion on framing machine learning problems. Data is emphasized as a critical component in machine learning; a high-quality algorithm with insufficient data will perform poorly, while even a less sophisticated algorithm can achieve good results with ample data. The upcoming videos will explore various methods for obtaining data, equipping learners with the ability to acquire necessary data for diverse problem statements. The data acquisition topic will be covered across four videos.\n",
    "\n",
    "**Data Sources to be Covered**\n",
    "\n",
    "The series will cover data acquisition in a step-by-step manner across four videos:\n",
    "\n",
    "* **Video 1 (This Video):** Working with CSV Files. This is the most common data format encountered initially in machine learning projects. Its straightforward structure, being comma-separated, makes it easier to work with compared to more complex formats.\n",
    "* **Video 2:** Focusing on JSON (JavaScript Object Notation) and SQL. JSON is another prevalent format, often used in APIs for data exchange between programming languages due to its universal acceptance. SQL is fundamental for interacting with databases.\n",
    "* **Video 3:** Fetching Data from APIs. This involves learning how to programmatically request and modify data from web server APIs, a crucial skill for real-world data acquisition.\n",
    "* **Video 4:** Web Scraping. This technique is employed when data is available on websites without a direct API, involving the use of parsers to navigate HTML code and extract relevant information.\n",
    "\n",
    "While these four methods form the primary focus, other data sources like Google BigQuery and data warehouses, as well as advanced SQL interactions, exist. However, mastering these initial four techniques is projected to cover approximately 90% of data acquisition needs in typical machine learning projects. Further learning can occur on an as-needed basis.\n",
    "\n",
    "**Working with CSV Files in Detail**\n",
    "\n",
    "The session utilizes a Jupyter Notebook environment with pre-uploaded CSV files for demonstration.\n",
    "\n",
    "**Understanding CSV Files**\n",
    "\n",
    "A CSV (Comma Separated Values) file is a plain text file where each line represents a row of data, and values within each row are separated by commas. This format is highly popular in machine learning and data science.\n",
    "\n",
    "**Tab Separated Values (TSV)**\n",
    "\n",
    "A variation, TSV (Tab Separated Values), uses tabs instead of commas as delimiters. Handling such files is also covered.\n",
    "\n",
    "**Loading CSV Files with Pandas `read_csv` Function**\n",
    "\n",
    "The primary tool for working with CSV files in Python is the `read_csv` function from the Pandas library. While a basic demonstration of this function is simple, its extensive parameters offer powerful capabilities for handling various CSV file scenarios. The goal is to provide a comprehensive understanding of these parameters for future reference. Approximately 15-16 parameters will be discussed.\n",
    "\n",
    "**Exploring `read_csv` Parameters**\n",
    "\n",
    "Referring to the Pandas documentation for `read_csv` reveals a multitude of parameters designed to accommodate diverse CSV file structures and complexities. Understanding these parameters can significantly streamline data loading and preprocessing. This session will focus on commonly encountered and practically relevant parameters.\n",
    "\n",
    "**Loading from Local Machine and URL**\n",
    "\n",
    "* **Local Files:** If the CSV file is located on your local machine, you can load it by providing its path to the `read_csv` function. If the file is in the same directory as your Python script or Jupyter Notebook, you can simply use the filename. Otherwise, provide the relative or absolute path.\n",
    "* **URLs:** To load a CSV file directly from a URL, you might need to use the `requests` library to fetch the file content. The fetched content can then be read using `io.StringIO` to simulate a file-like object for `read_csv`. A code snippet for this process is provided, involving importing the `requests` library, making a GET request to the URL, and then reading the text content.\n",
    "\n",
    "**Key Parameters Explained**\n",
    "\n",
    "* **`sep` (Separator):** This parameter specifies the delimiter used in the CSV file. The default is a comma (,). For files using different delimiters, such as tabs for TSV files, this parameter needs to be adjusted accordingly (e.g., `sep='\\t'` for tab). Failure to specify the correct separator will result in the entire row being read as a single column.\n",
    "* **`names`:** When a CSV file lacks a header row containing column names, the `names` parameter allows you to provide a list of column names. Pandas will then use this list as the header, and the first row of data will be treated as the first data entry.\n",
    "* **`index_col` (Index Column):** This parameter allows you to designate one of the columns in the CSV file as the index of the resulting DataFrame. Instead of the default numerical index, the specified column's values will serve as the row labels.\n",
    "* **`header`:** This parameter specifies which row in the CSV file should be treated as the header row containing column names. The default is `header=0`, indicating the first row. If the header is in a different row (e.g., the second row), you would set `header=1` (using 0-based indexing). Setting `header=None` indicates that the file has no header row, in which case you might want to use the `names` parameter.\n",
    "* **`usecols` (Use Columns):** To load only a subset of columns from a large CSV file, the `usecols` parameter can be used. You can provide either a list of column names or a list of column indices (0-based) to specify which columns to include in the resulting DataFrame. This can improve efficiency by reducing memory usage and processing time.\n",
    "* **`squeeze`:** If the loaded data contains only one column, setting `squeeze=True` will return a Pandas Series instead of a DataFrame. This can be useful for simplifying the data structure when dealing with single-column datasets.\n",
    "* **`skiprows`:** This parameter allows you to skip a specified number of rows at the beginning of the CSV file. You can provide an integer to skip that many initial rows or a list of row numbers (0-based) to skip specific rows. Additionally, you can pass a function to `skiprows`. This function will be evaluated for each row index, and if it returns `True`, the row will be skipped.\n",
    "* **`nrows` (Number of Rows):** To read only a limited number of rows from a CSV file, the `nrows` parameter can be used. This is particularly helpful when working with very large files and you only need a sample of the data for initial exploration or testing.\n",
    "* **`encoding`:** This parameter specifies the character encoding of the CSV file. The default is usually UTF-8, which works for most standard text files. However, some files might use different encodings (e.g., Latin-1, ISO-8859-1). Incorrectly specifying the encoding can lead to errors or misinterpretation of characters. If you encounter issues with special characters, you might need to try different encodings.\n",
    "* **`error_bad_lines`:** By default, `read_csv` will raise an error if it encounters a line with too many fields (more than the number of columns). Setting `error_bad_lines=False` will cause these \"bad\" lines to be skipped without raising an error.\n",
    "* **`dtype` (Data Type):** This parameter allows you to explicitly specify the data type for one or more columns. You can pass a dictionary where keys are column names and values are the desired data types (e.g., `'int'`, `'float'`, `'str'`). This can be useful for optimizing memory usage or ensuring that columns are interpreted correctly (e.g., treating a column that might be inferred as a float as an integer).\n",
    "* **`parse_dates`:** If your CSV file contains columns representing dates, the `parse_dates` parameter can automatically convert these string representations into datetime objects. You can provide a list of column names or indices to be parsed as dates. Additionally, you can specify a dictionary to combine multiple columns into a single datetime column. The keys of the dictionary would be the new datetime column name, and the values would be lists of the columns to combine (e.g., `{'Date': ['Year', 'Month', 'Day']}`).\n",
    "* **`converters`:** This parameter allows you to apply custom functions to the values in specific columns during the reading process. You can pass a dictionary where keys are column names and values are the functions to be applied to those columns. This is useful for performing transformations or cleaning data as it's being loaded.\n",
    "* **`na_values` (NA Values):** This parameter specifies which strings or other values should be treated as missing values (NaN - Not a Number). You can provide a single string, a list of strings, or a dictionary where keys are column names and values are the strings to treat as NA in those specific columns.\n",
    "* **`chunksize`:** For very large CSV files that cannot fit into memory at once, the `chunksize` parameter allows you to read the file in smaller chunks. When you specify a `chunksize` (an integer representing the number of rows per chunk), `read_csv` returns a `TextFileReader` object, which is an iterable. You can then loop through this iterable to process the data chunk by chunk, reducing memory usage. Each chunk is a DataFrame.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "This session provided a comprehensive overview of working with CSV files using the Pandas `read_csv` function and its numerous parameters. Understanding and effectively utilizing these parameters is crucial for handling diverse CSV file formats and efficiently loading data for machine learning tasks. The next session will focus on working with JSON files and SQL databases."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
