{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036f8ccd",
   "metadata": {},
   "source": [
    "Web scraping is the process of extracting data from websites automatically using a computer program or script. Here's a structured and complete explanation from **beginner to advanced level**, including concepts, tools, techniques, best practices, and ethical/legal aspects.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”° BEGINNER LEVEL\n",
    "\n",
    "### 1. **What is Web Scraping?**\n",
    "\n",
    "Web scraping is the automated method of collecting data from web pages. This is typically done using a program that sends requests to a website, retrieves the HTML, and extracts useful information.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Basic Terminology**\n",
    "\n",
    "* **HTML**: The language used to create webpages. Scrapers parse HTML to extract data.\n",
    "* **DOM (Document Object Model)**: The tree-like structure of an HTML page.\n",
    "* **Selectors**: CSS or XPath expressions used to locate elements in the DOM.\n",
    "* **Request**: An HTTP call sent to a website (e.g., GET, POST).\n",
    "* **Response**: The HTML or data returned from the server.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Common Libraries and Tools**\n",
    "\n",
    "* **Python**: Most popular language for web scraping.\n",
    "* **Libraries**:\n",
    "\n",
    "  * `requests`: To fetch the webpage.\n",
    "  * `BeautifulSoup`: To parse HTML and extract data.\n",
    "  * `lxml`: A fast and powerful parser.\n",
    "  * `Selenium`: For scraping dynamic JavaScript-loaded content.\n",
    "  * `Scrapy`: A full-featured web scraping framework.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Example (Using `requests` + `BeautifulSoup`)**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract all headings\n",
    "headings = soup.find_all('h1')\n",
    "for h in headings:\n",
    "    print(h.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Ethical and Legal Considerations**\n",
    "\n",
    "* Check the websiteâ€™s `robots.txt` file (e.g., `https://example.com/robots.txt`).\n",
    "* Respect terms of service.\n",
    "* Avoid overloading servers.\n",
    "* Donâ€™t scrape personal data without consent.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ INTERMEDIATE LEVEL\n",
    "\n",
    "### 1. **Working with XPath and CSS Selectors**\n",
    "\n",
    "* **CSS Selector Example**:\n",
    "\n",
    "  ```python\n",
    "  soup.select('div.article > h2')\n",
    "  ```\n",
    "* **XPath Example** (with `lxml` or `Selenium`):\n",
    "\n",
    "  ```python\n",
    "  tree.xpath('//div[@class=\"article\"]/h2/text()')\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Handling Pagination**\n",
    "\n",
    "Web data is often spread across multiple pages:\n",
    "\n",
    "```python\n",
    "for page in range(1, 5):\n",
    "    url = f'https://example.com/page={page}'\n",
    "    response = requests.get(url)\n",
    "    # process each page\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Dealing with JavaScript-Rendered Content**\n",
    "\n",
    "Use **Selenium** to control a real browser:\n",
    "\n",
    "```python\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://example.com')\n",
    "content = driver.page_source\n",
    "```\n",
    "\n",
    "Or use **requests-html**:\n",
    "\n",
    "```python\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "session = HTMLSession()\n",
    "r = session.get('https://example.com')\n",
    "r.html.render()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Saving Extracted Data**\n",
    "\n",
    "* CSV\n",
    "* JSON\n",
    "* SQLite / PostgreSQL / MySQL\n",
    "\n",
    "```python\n",
    "import csv\n",
    "\n",
    "with open('data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Date'])\n",
    "    writer.writerow(['Example Title', '2025-05-08'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ ADVANCED LEVEL\n",
    "\n",
    "### 1. **Scrapy Framework**\n",
    "\n",
    "Scrapy is a powerful tool for large-scale scraping.\n",
    "\n",
    "```bash\n",
    "scrapy startproject myproject\n",
    "```\n",
    "\n",
    "Define a Spider:\n",
    "\n",
    "```python\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'example'\n",
    "    start_urls = ['https://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for title in response.css('h2.title'):\n",
    "            yield {'title': title.css('::text').get()}\n",
    "```\n",
    "\n",
    "Run it:\n",
    "\n",
    "```bash\n",
    "scrapy crawl example\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Session Handling, Cookies, and Headers**\n",
    "\n",
    "To mimic a real browser:\n",
    "\n",
    "```python\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "cookies = {'sessionid': 'abc123'}\n",
    "\n",
    "r = requests.get(url, headers=headers, cookies=cookies)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Handling Captchas**\n",
    "\n",
    "* Use services like **2Captcha**, **AntiCaptcha**, or **Playwright**.\n",
    "* Detect captcha using image analysis or specific DOM patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Rotating Proxies and User-Agents**\n",
    "\n",
    "Avoid blocking:\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 ...',\n",
    "    'Chrome/98.0 ...',\n",
    "]\n",
    "\n",
    "headers = {'User-Agent': random.choice(user_agents)}\n",
    "```\n",
    "\n",
    "Use proxy:\n",
    "\n",
    "```python\n",
    "proxies = {'http': 'http://proxy_ip:port'}\n",
    "requests.get(url, headers=headers, proxies=proxies)\n",
    "```\n",
    "\n",
    "Use proxy rotation libraries like:\n",
    "\n",
    "* `scrapy-rotating-proxies`\n",
    "* `proxy-pool`\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Rate Limiting and Throttling**\n",
    "\n",
    "Add delays:\n",
    "\n",
    "```python\n",
    "import time\n",
    "time.sleep(2)  # wait 2 seconds between requests\n",
    "```\n",
    "\n",
    "Use Scrapyâ€™s built-in auto-throttle:\n",
    "\n",
    "```python\n",
    "AUTOTHROTTLE_ENABLED = True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Storing Data in Databases**\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('data.db')\n",
    "c = conn.cursor()\n",
    "c.execute('CREATE TABLE IF NOT EXISTS articles (title TEXT)')\n",
    "c.execute('INSERT INTO articles VALUES (?)', ('Example Title',))\n",
    "conn.commit()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Using Headless Browsers**\n",
    "\n",
    "Run browsers without a GUI:\n",
    "\n",
    "```python\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Using APIs Instead of Scraping**\n",
    "\n",
    "Whenever possible, **prefer public APIs** for data instead of scraping HTML, as they are:\n",
    "\n",
    "* Faster\n",
    "* More reliable\n",
    "* Less likely to break\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Deployment**\n",
    "\n",
    "* Run scripts on a schedule using **cron jobs** (Linux) or **Task Scheduler** (Windows).\n",
    "* Use cloud services like:\n",
    "\n",
    "  * AWS Lambda\n",
    "  * Heroku\n",
    "  * DigitalOcean\n",
    "  * Render\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Best Practices\n",
    "\n",
    "* Respect `robots.txt` and Terms of Use.\n",
    "* Use delays and throttling.\n",
    "* Use retry logic.\n",
    "* Structure your scraper for maintainability.\n",
    "* Use logs and error handling.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Suggested Learning Path\n",
    "\n",
    "1. Python basics (if not known)\n",
    "2. Learn HTML & CSS\n",
    "3. Practice with `requests` and `BeautifulSoup`\n",
    "4. Learn `Selenium` for JavaScript content\n",
    "5. Dive into `Scrapy` for large-scale projects\n",
    "6. Explore advanced techniques: headless browsers, proxies, captchas\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
