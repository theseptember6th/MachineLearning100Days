{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1376d1f0",
   "metadata": {},
   "source": [
    "Here’s an organized, in‑depth set of study notes on **tensors**, distilled from the lecture and structured for clarity:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What Is a Tensor?\n",
    "\n",
    "- **General definition**: A tensor is a data structure (container) for storing numbers (scalars, vectors, matrices, higher‑order arrays) along one or more axes (dimensions).\n",
    "- **Key idea**: Tensors generalize scalars (0‑D), vectors (1‑D), and matrices (2‑D) to _n_‑dimensional arrays.\n",
    "  - 0‑D: **Scalar** (single number)  \n",
    "  - 1‑D: **Vector** (list of numbers)  \n",
    "  - 2‑D: **Matrix** (grid of numbers)  \n",
    "  - 3‑D and above: Higher‑order tensor (e.g. a “cube” of numbers, image data, video frames, time‑series stacks, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why Tensors in Machine Learning?\n",
    "\n",
    "1. **Uniform data representation**  \n",
    "   – All inputs to ML models—tabular features, text embeddings, audio spectrograms, images, video—can be represented as tensors.  \n",
    "2. **Library support**  \n",
    "   – Frameworks like TensorFlow, PyTorch, JAX, NumPy use tensors as their fundamental data type and enable optimized operations (GPU/TPU).  \n",
    "3. **Performance & scaling**  \n",
    "   – Tensor operations (e.g., dot products, convolutions) are highly optimized in C/C++ and parallelizable across hardware.  \n",
    "4. **Deep learning architectures**  \n",
    "   – Neural networks transform input tensors through layers (linear maps, convolutions, attention), producing output tensors.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Tensor Terminology\n",
    "\n",
    "| Term             | Meaning                                                             |\n",
    "| ---------------- | ------------------------------------------------------------------- |\n",
    "| **Rank**         | Number of dimensions (axes) of the tensor (e.g. rank = 3 for a 3‑D tensor) |\n",
    "| **Shape**        | Size along each axis (e.g. `(height, width, channels)` for an image tensor) |\n",
    "| **Axes / Modes** | Each dimension; sometimes called modes (e.g. time axis, feature axis) |\n",
    "| **Elements**     | Individual numeric entries within the tensor                        |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Creating Tensors in Python\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Scalar (0‑D)\n",
    "s = np.array(42)\n",
    "\n",
    "# Vector (1‑D)\n",
    "v = np.array([1, 2, 3])\n",
    "\n",
    "# Matrix (2‑D)\n",
    "M = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "# 3‑D tensor (e.g. image: height×width×channels)\n",
    "img = np.zeros((256, 256, 3))\n",
    "\n",
    "# 4‑D tensor (e.g. batch of images: batch_size×H×W×C)\n",
    "batch = np.zeros((32, 256, 256, 3))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Common Tensor Operations\n",
    "\n",
    "1. **Indexing & slicing**: pick out subtensors  \n",
    "2. **Reshape**: change shape without copying data  \n",
    "3. **Transpose / permute**: reorder axes  \n",
    "4. **Elementwise ops**: add, multiply, apply activation functions  \n",
    "5. **Reduction**: sum, mean along axes  \n",
    "6. **Linear algebra**: dot product, matrix multiply (`@`), eigen-decomposition  \n",
    "7. **Broadcasting**: implicit expansion of smaller tensors to match shapes\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Examples in ML Context\n",
    "\n",
    "- **Tabular data**:  \n",
    "  – Shape `(N, D)` where N = samples, D = features  \n",
    "- **Text**:  \n",
    "  – Word embeddings: `(seq_len, embed_dim)`  \n",
    "  – Batch of sentences: `(batch, seq_len, embed_dim)`  \n",
    "- **Image**:  \n",
    "  – Single image: `(H, W, C)`  \n",
    "  – Batch: `(batch, H, W, C)`  \n",
    "- **Video**:  \n",
    "  – `(batch, frames, H, W, C)` → rank 5 tensor  \n",
    "- **Time series**:  \n",
    "  – `(batch, timesteps, features)`\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why “Tensor” vs. Just “Array”?\n",
    "\n",
    "- Historically from physics & differential geometry: a tensor transforms predictably under coordinate changes.  \n",
    "- In ML libraries, “tensor” simply means “n‑dimensional array” with attached gradient‑tracking metadata for autodiff.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "1. **Tensors** = multi‑dimensional numeric arrays  \n",
    "2. They power all data representations in ML/deep learning  \n",
    "3. Understanding shape, rank, axes, and operations on tensors is fundamental  \n",
    "4. Frameworks provide efficient, GPU‑accelerated tensor operations  \n",
    "\n",
    "---\n",
    "\n",
    "> **Next Steps**:  \n",
    "> - Hands‑on: play with `torch.Tensor` or `tf.Tensor` in a notebook  \n",
    "> - Visualize tensor shapes as you pass data through a simple neural network  \n",
    "> - Explore advanced tensor factorizations (e.g. CP, Tucker) for compression  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
